{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 17 files: 100%|████████████████████| 17/17 [00:00<00:00, 101571.46it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "no <image> tag found in input. Automatically append one at the beginning of text.\n",
      "input:  <image>\n",
      "<image>\n",
      "<image>\n",
      "<image>\n",
      "<image>\n",
      "<image>\n",
      "<video>\\n Please describe this video.\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is vicuna_v1, using vicuna_v1\n",
      "torch.Size([6, 3, 384, 384])\n",
      "The video opens with a woman screaming in terror as a large, dark creature approaches her. The creature has sharp teeth and is growling, indicating that it is angry or aggressive. The woman is wearing a dark-colored dress and is being held by a man who is also screaming in fear. The scene then cuts to a close-up of the creature's face, revealing its sharp teeth and menacing expression. The camera then shifts to a wide shot of the two characters running away from the creature, with the man and woman running side by side. The scene is set in a dark, forested area with bare trees, and the sky is overcast. The characters are running at a fast pace, with the man holding the woman close to his body for protection. The video ends with the characters running away from the creature, who is still growling and roaring.\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore llava/eval/run_vila.py \\\n",
    "    --model-path Efficient-Large-Model/VILA1.5-3b \\\n",
    "    --conv-mode vicuna_v1 \\\n",
    "    --query \"<video>\\n Please describe this video.\" \\\n",
    "    --video-file /data/dataset/VidSitu/data/vsitu_video_trimmed_dir/v_104ZQtfYDso_seg_35_45.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"Ev1\": {\"Args\": {\"Arg1 (thing collapsing)\": \"A man in black suit and blue shirt\", \"ArgM (location)\": \"on the ground\", \"ArgM (manner)\": \"wounded on his stomach\"}, \"Verb\": \"collapse (fall down)\"}, \n",
    " \"Ev2\": {\"Args\": {\"Arg1 (agonized entity)\": \"A man in black suit and blue shirt\", \"Scene of the Event\": \"on the ground\"}, \"Verb\": \"agonize (suffer distress)\"}, \n",
    " \"Ev3\": {\"Args\": {\"Arg0 (agent, causer)\": \"An old woman who is crossing the road\", \"Arg1 (entity moving)\": \"A man in black leather jacket\", \"Arg2 (person waved at)\": \"An old woman holding crutches\", \"ArgM (direction)\": \"In the middle of the road\", \"Scene of the Event\": \"On the road\"}, \"Verb\": \"wave (to signal (a person) or move back and forth)\"}, \n",
    " \"Ev4\": {\"Args\": {\"Arg1 (entity in motion)\": \"A man in black leather jacket\", \"Arg2 (destination)\": \"An old woman holding crutches\", \"ArgM (direction)\": \"In the middle of the road\", \"ArgM (manner)\": \"While telling her to get out of the street\", \"Scene of the Event\": \"On the road\"}, \"Verb\": \"approach (move towards)\"}, \n",
    " \"Ev5\": {\"Args\": {\"Arg0 (walker)\": \"A man in black leather jacket\", \"ArgM (direction)\": \"An old woman holding crutches\", \"ArgM (manner)\": \"While holding a black bag\", \"Scene of the Event\": \"On the road\"}, \"Arg_List\": {\"Arg0 (walker)\": \"0\", \"ArgM (direction)\": \"1\", \"ArgM (manner)\": \"2\", \"Scene of the Event\": \"3\"}, \"Verb\": \"walk (walk)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: input:  Given a 10 second video, provide a structured situational summary for 5 events \"Ev1\",\"Ev2\",\"Ev3\",\"Ev4\",\"Ev5\", with each event spanning 2 seconds, into the JSON response format template below, that captures the essence of the scene. Specifically, for each event describe an action or activity is taking place in (Verb). Then fill the nouns playing the roles \"Arg0\", \"Arg1\", \"Arg2\", \"Scene of the Event\" in the verb (action) describing the event .\n",
      "JSON Response Format Template: {{\n",
      "  \"Ev1\": {{\n",
      "    \"Args\": {{ \"Arg0\": {}, \"Arg1\": {}, \"Arg2\": {}, \"Scene of the Event\": {} }}, \n",
      "    \"Verb\": {}\n",
      "  }} \n",
      "  \"Ev2\": {{\n",
      "    \"Args\": {{ \"Arg0\": {}, \"Arg1\": {}, \"Arg2\": {}, \"Scene of the Event\": {} }}, \n",
      "    \"Verb\": {}\n",
      "  }}\n",
      "  \"Ev3\": {{\n",
      "    \"Args\": {{ \"Arg0\": {}, \"Arg1\": {}, \"Arg2\": {}, \"Scene of the Event\": {} }}, \n",
      "    \"Verb\": {}\n",
      "  }}\n",
      "  \"Ev4\": {{\n",
      "    \"Args\": {{ \"Arg0\": {}, \"Arg1\": {}, \"Arg2\": {}, \"Scene of the Event\": {} }}, \n",
      "    \"Verb\": {}\n",
      "  }}\n",
      "  \"Ev5\": {{\n",
      "    \"Args\": {{ \"Arg0\": {}, \"Arg1\": {}, \"Arg2\": {}, \"Scene of the Event\": {} }}, \n",
      "    \"Verb\": {}\n",
      "  }}\n",
      "}}\n",
      "\n",
      "In-Context Example for Guidance: {{\n",
      "  \"Ev1\": {{\n",
      "    \"Args\": {{ \"Arg1 (thing collapsing)\": \"A man in black suit and blue shirt\", \"Scene of the Event\": \"on the ground\" }}, \n",
      "    \"Verb\": \"collapse (fall down)\" \n",
      "  }}, \n",
      "  \"Ev2\": {{\n",
      "    \"Args\": {{ \"Arg1 (agonized entity)\": \"A man in black suit and blue shirt\", \"Scene of the Event\": \"on the ground\" }}, \n",
      "    \"Verb\": \"agonize (suffer distress)\" \n",
      "  }}, \n",
      "  \"Ev3\": {{\n",
      "    \"Args\": {{ \"Arg0 (agent, causer)\": \"An old woman who is crossing the road\", \"Scene of the Event\": \"A man in black leather jacket\", \"Arg2 (person waved at)\": \"An old woman holding crutches\", \"Scene of the Event\": \"On the road\" }},\n",
      "    \"Verb\": \"wave (to signal (a person) or move back and forth)\" \n",
      "  }}, \n",
      "  \"Ev4\": {{\n",
      "    \"Args\": {{ \"Arg1 (entity in motion)\": \"A man in black leather jacket\", \"Arg2 (destination)\": \"An old woman holding crutches\", \"Scene of the Event\": \"On the road\" }}, \n",
      "    \"Verb\": \"approach (move towards)\" \n",
      "  }}, \n",
      "  \"Ev5\": {{\n",
      "    \"Args\": {{ \"Arg0 (walker)\": \"A man in black leather jacket\", \"Scene of the Event\": \"On the road\" }}, \n",
      "    \"Verb\": \"walk (walk)\" \n",
      "  }},\n",
      "}}\n",
      "\n",
      "Do not give an explanation or any more unnessary information other than the JSON format.\n",
      "Now, based on the image <image>, provide a structured summary in the JSON format as instructed above.\n",
      "\n",
      "[WARNING] the auto inferred conversation mode is llava_v0, while `--conv-mode` is vicuna_v1, using vicuna_v1\n",
      "torch.Size([6, 3, 384, 384])\n",
      "{{\n",
      "  \"Ev1\": {{\n",
      "    \"Args\": {{ \"Arg0 (walker)\": \"A man in black leather jacket\", \"Scene of the Event\": \"On the road\" }}, \n",
      "    \"Verb\": \"walk (walk)\" \n",
      "  }}\n",
      "}}\n",
      "\n",
      "Error: \n",
      "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]\n",
      "Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 97275.81it/s]\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.88s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "json_visual_prompt = \"\"\"Given the image, provide a structured situational summary following the JSON response format template, that captures the essence of the scene. Specifically, describe the action (verb). Then, fill the nouns playing the roles \"Arg0\", \"Arg1\", \"Arg2\", \"Scene of the Event\" in the verb (action).\n",
    "JSON Response Format Template: {{\n",
    "    \"Args\": {{ \"Arg0\": {}, \"Arg1\": {}, \"Arg2\": {}, \"Scene of the Event\": {} }}, \n",
    "    \"Verb\": {}\n",
    "}}\n",
    "\n",
    "In-Context Example for Guidance: {{\n",
    "<image>\n",
    "  \"Verb\": \"collapse (fall down)\", \n",
    "  \"Args\": {{ \"Arg1 (thing collapsing)\": \"A man in black suit and blue shirt\", \"Scene of the Event\": \"on the ground\" }}, \n",
    "    \n",
    "    \n",
    "<image> \n",
    "  \"Verb\": \"agonize (suffer distress)\",\n",
    "  \"Args\": {{ \"Arg1 (agonized entity)\": \"A man in black suit and blue shirt\", \"Scene of the Event\": \"on the ground\" }}, \n",
    "  \n",
    "\n",
    "<image>\n",
    "  \"Verb\": \"wave (to signal (a person) or move back and forth)\", \n",
    "  \"Args\": {{ \"Arg0 (agent, causer)\": \"An old woman who is crossing the road\", \"Scene of the Event\": \"A man in black leather jacket\", \"Arg2 (person waved at)\": \"An old woman holding crutches\", \"Scene of the Event\": \"On the road\" }},\n",
    "    \n",
    "\n",
    "<image>\n",
    "  \"Verb\": \"approach (move towards)\",\n",
    "  \"Args\": {{ \"Arg1 (entity in motion)\": \"A man in black leather jacket\", \"Arg2 (destination)\": \"An old woman holding crutches\", \"Scene of the Event\": \"On the road\" }}, \n",
    "     \n",
    "\n",
    "<image>\n",
    "  \"Verb\": \"walk (walk)\",  \n",
    "  \"Args\": {{ \"Arg0 (walker)\": \"A man in black leather jacket\", \"Scene of the Event\": \"On the road\" }}  \n",
    "}}\n",
    "\n",
    "Do not give an explanation or any more unnessary information other than the JSON format.\n",
    "Now, based on the image <image>, provide a structured summary in the JSON format as instructed above.\n",
    "\"\"\"\n",
    "\n",
    "video_files = \"\"\n",
    "image_files = \"vsitu_11_frames_per_vid/v_VYQoxBs5N2A_seg_90_100/frame_01.jpg,\\\n",
    "               vsitu_11_frames_per_vid/v_VYQoxBs5N2A_seg_90_100/frame_03.jpg,\\\n",
    "               vsitu_11_frames_per_vid/v_VYQoxBs5N2A_seg_90_100/frame_05.jpg,\\\n",
    "               vsitu_11_frames_per_vid/v_VYQoxBs5N2A_seg_90_100/frame_07.jpg,\\\n",
    "               vsitu_11_frames_per_vid/v_VYQoxBs5N2A_seg_90_100/frame_09.jpg,\\\n",
    "               vsitu_11_frames_per_vid/v_VYQoxBs5N2A_seg_120_130/frame_01.jpg\"\n",
    "\n",
    "# Prepare the command\n",
    "command = [\n",
    "    \"python\", \"-W\", \"ignore\", \"./llava/eval/run_vila.py\",\n",
    "    \"--model-path\", \"Efficient-Large-Model/Llama-3-VILA1.5-8b\",\n",
    "    \"--conv-mode\", \"llama_3\",\n",
    "    \"--query\", json_visual_prompt,\n",
    "    \"--image-file\", image_files\n",
    "]\n",
    "\n",
    "# Run the command using subprocess\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the output and error\n",
    "print(\"Output:\", result.stdout)\n",
    "print(\"Error:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29096"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "len(os.listdir(\"../vidsitu_data/vsitu_11_frames_per_vid\"))\n",
    "#vidsitu_data/vsitu_11_frames_per_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidsitu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
